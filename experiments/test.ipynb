{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "import safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = load_file(\"/storage/lcm/Wan-Distill/Spark/14B_32_16/checkpoint-200/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.blocks.0.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.0.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.0.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.0.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.0.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.0.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.0.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.0.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.0.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.0.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.0.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.0.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.0.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.0.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.0.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.0.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.0.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.0.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.0.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.0.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.1.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.1.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.1.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.1.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.1.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.1.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.1.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.1.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.1.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.1.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.1.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.1.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.1.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.1.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.1.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.1.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.1.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.1.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.1.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.1.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.10.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.10.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.10.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.10.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.10.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.10.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.10.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.10.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.10.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.10.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.10.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.10.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.10.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.10.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.10.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.10.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.10.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.10.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.10.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.10.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.11.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.11.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.11.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.11.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.11.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.11.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.11.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.11.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.11.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.11.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.11.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.11.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.11.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.11.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.11.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.11.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.11.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.11.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.11.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.11.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.12.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.12.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.12.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.12.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.12.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.12.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.12.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.12.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.12.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.12.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.12.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.12.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.12.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.12.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.12.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.12.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.12.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.12.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.12.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.12.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.13.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.13.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.13.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.13.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.13.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.13.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.13.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.13.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.13.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.13.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.13.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.13.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.13.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.13.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.13.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.13.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.13.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.13.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.13.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.13.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.14.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.14.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.14.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.14.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.14.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.14.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.14.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.14.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.14.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.14.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.14.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.14.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.14.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.14.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.14.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.14.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.14.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.14.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.14.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.14.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.15.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.15.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.15.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.15.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.15.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.15.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.15.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.15.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.15.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.15.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.15.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.15.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.15.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.15.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.15.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.15.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.15.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.15.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.15.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.15.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.16.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.16.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.16.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.16.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.16.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.16.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.16.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.16.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.16.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.16.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.16.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.16.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.16.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.16.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.16.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.16.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.16.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.16.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.16.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.16.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.17.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.17.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.17.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.17.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.17.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.17.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.17.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.17.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.17.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.17.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.17.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.17.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.17.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.17.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.17.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.17.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.17.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.17.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.17.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.17.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.18.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.18.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.18.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.18.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.18.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.18.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.18.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.18.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.18.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.18.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.18.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.18.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.18.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.18.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.18.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.18.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.18.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.18.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.18.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.18.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.19.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.19.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.19.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.19.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.19.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.19.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.19.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.19.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.19.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.19.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.19.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.19.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.19.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.19.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.19.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.19.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.19.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.19.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.19.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.19.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.2.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.2.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.2.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.2.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.2.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.2.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.2.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.2.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.2.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.2.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.2.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.2.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.2.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.2.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.2.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.2.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.2.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.2.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.2.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.2.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.20.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.20.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.20.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.20.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.20.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.20.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.20.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.20.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.20.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.20.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.20.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.20.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.20.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.20.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.20.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.20.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.20.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.20.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.20.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.20.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.21.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.21.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.21.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.21.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.21.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.21.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.21.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.21.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.21.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.21.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.21.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.21.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.21.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.21.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.21.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.21.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.21.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.21.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.21.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.21.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.22.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.22.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.22.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.22.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.22.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.22.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.22.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.22.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.22.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.22.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.22.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.22.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.22.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.22.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.22.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.22.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.22.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.22.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.22.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.22.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.23.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.23.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.23.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.23.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.23.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.23.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.23.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.23.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.23.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.23.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.23.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.23.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.23.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.23.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.23.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.23.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.23.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.23.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.23.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.23.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.24.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.24.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.24.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.24.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.24.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.24.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.24.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.24.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.24.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.24.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.24.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.24.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.24.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.24.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.24.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.24.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.24.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.24.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.24.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.24.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.25.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.25.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.25.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.25.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.25.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.25.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.25.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.25.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.25.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.25.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.25.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.25.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.25.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.25.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.25.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.25.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.25.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.25.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.25.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.25.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.26.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.26.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.26.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.26.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.26.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.26.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.26.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.26.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.26.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.26.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.26.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.26.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.26.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.26.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.26.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.26.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.26.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.26.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.26.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.26.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.27.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.27.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.27.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.27.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.27.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.27.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.27.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.27.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.27.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.27.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.27.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.27.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.27.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.27.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.27.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.27.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.27.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.27.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.27.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.27.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.28.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.28.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.28.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.28.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.28.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.28.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.28.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.28.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.28.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.28.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.28.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.28.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.28.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.28.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.28.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.28.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.28.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.28.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.28.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.28.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.29.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.29.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.29.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.29.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.29.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.29.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.29.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.29.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.29.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.29.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.29.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.29.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.29.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.29.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.29.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.29.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.29.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.29.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.29.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.29.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.3.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.3.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.3.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.3.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.3.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.3.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.3.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.3.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.3.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.3.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.3.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.3.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.3.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.3.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.3.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.3.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.3.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.3.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.3.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.3.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.30.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.30.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.30.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.30.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.30.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.30.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.30.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.30.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.30.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.30.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.30.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.30.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.30.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.30.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.30.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.30.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.30.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.30.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.30.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.30.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.31.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.31.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.31.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.31.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.31.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.31.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.31.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.31.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.31.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.31.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.31.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.31.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.31.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.31.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.31.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.31.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.31.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.31.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.31.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.31.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.32.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.32.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.32.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.32.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.32.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.32.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.32.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.32.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.32.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.32.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.32.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.32.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.32.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.32.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.32.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.32.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.32.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.32.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.32.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.32.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.33.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.33.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.33.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.33.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.33.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.33.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.33.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.33.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.33.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.33.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.33.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.33.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.33.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.33.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.33.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.33.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.33.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.33.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.33.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.33.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.34.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.34.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.34.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.34.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.34.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.34.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.34.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.34.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.34.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.34.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.34.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.34.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.34.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.34.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.34.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.34.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.34.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.34.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.34.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.34.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.35.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.35.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.35.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.35.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.35.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.35.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.35.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.35.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.35.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.35.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.35.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.35.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.35.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.35.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.35.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.35.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.35.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.35.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.35.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.35.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.36.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.36.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.36.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.36.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.36.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.36.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.36.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.36.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.36.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.36.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.36.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.36.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.36.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.36.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.36.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.36.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.36.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.36.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.36.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.36.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.37.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.37.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.37.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.37.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.37.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.37.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.37.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.37.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.37.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.37.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.37.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.37.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.37.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.37.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.37.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.37.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.37.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.37.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.37.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.37.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.38.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.38.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.38.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.38.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.38.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.38.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.38.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.38.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.38.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.38.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.38.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.38.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.38.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.38.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.38.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.38.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.38.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.38.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.38.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.38.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.39.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.39.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.39.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.39.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.39.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.39.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.39.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.39.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.39.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.39.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.39.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.39.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.39.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.39.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.39.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.39.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.39.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.39.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.39.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.39.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.4.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.4.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.4.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.4.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.4.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.4.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.4.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.4.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.4.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.4.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.4.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.4.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.4.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.4.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.4.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.4.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.4.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.4.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.4.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.4.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.5.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.5.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.5.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.5.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.5.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.5.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.5.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.5.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.5.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.5.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.5.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.5.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.5.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.5.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.5.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.5.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.5.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.5.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.5.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.5.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.6.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.6.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.6.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.6.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.6.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.6.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.6.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.6.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.6.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.6.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.6.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.6.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.6.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.6.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.6.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.6.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.6.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.6.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.6.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.6.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.7.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.7.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.7.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.7.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.7.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.7.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.7.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.7.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.7.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.7.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.7.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.7.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.7.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.7.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.7.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.7.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.7.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.7.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.7.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.7.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.8.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.8.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.8.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.8.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.8.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.8.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.8.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.8.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.8.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.8.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.8.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.8.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.8.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.8.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.8.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.8.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.8.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.8.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.8.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.8.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.9.attn1.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.9.attn1.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.9.attn1.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.9.attn1.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.9.attn1.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.9.attn1.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.9.attn1.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.9.attn1.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.9.attn2.to_k.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.9.attn2.to_k.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.9.attn2.to_out.0.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.9.attn2.to_out.0.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.9.attn2.to_q.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.9.attn2.to_q.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.9.attn2.to_v.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.9.attn2.to_v.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.blocks.9.ffn.net.0.proj.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.blocks.9.ffn.net.0.proj.lora_B.default.weight\n",
      "torch.Size([13824, 64])\n",
      "base_model.model.blocks.9.ffn.net.2.lora_A.default.weight\n",
      "torch.Size([64, 13824])\n",
      "base_model.model.blocks.9.ffn.net.2.lora_B.default.weight\n",
      "torch.Size([5120, 64])\n",
      "base_model.model.proj_out.lora_A.default.weight\n",
      "torch.Size([64, 5120])\n",
      "base_model.model.proj_out.lora_B.default.weight\n",
      "torch.Size([64, 64])\n"
     ]
    }
   ],
   "source": [
    "for key, value in state_dict.items():\n",
    "    if \"lora\" in key:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_dict = {}\n",
    "for key, value in state_dict.items():\n",
    "    new_key = key.replace(\"default.\", \"\")\n",
    "    new_state_dict[new_key] = value\n",
    "\n",
    "safetensors.torch.save_file(\n",
    "    new_state_dict,\n",
    "    \"/storage/lcm/Wan-Distill/Spark/14B_32_16/checkpoint-600/adapter_model.safetensors\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wan_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
